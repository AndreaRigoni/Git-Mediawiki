#! /usr/bin/perl

use strict;
use Switch;
use MediaWiki::API;
use Storable qw(freeze thaw);
use DateTime::Format::ISO8601;
use Encode qw(encode_utf8);

my $remotename = $ARGV[0];
my $url = $ARGV[1];

print STDERR "$url\n";

# commands parser
my $loop = 1;
my $entry;
my @cmd;
while ($loop) {
	$| = 1; #flush stdout
	$entry = <STDIN>;
	print STDERR $entry;
	chomp($entry);
	@cmd = undef;
	@cmd = split(/ /,$entry);
	switch ($cmd[0]) {
		case "capabilities" {
			if ($cmd[1] eq "") {
				mw_capabilities();
			}
		}
		case "list" {
			if ($cmd[2] eq "") {
				mw_list($cmd[1]);
			} 
		}
		case "import" {
			if ($cmd[1] ne "" && $cmd[2] eq "") {
				mw_import($url);
			} 
		}
		case "option" {
			mw_option($cmd[1],$cmd[2]);
		}
		case "fetch" {
			mw_fetch($url);
		}
		case "push" {
			#check the pattern +<src>:<dist>
			my @pushargs = split(/:/,$url);
			if ($pushargs[1] ne "" && $pushargs[2] eq "") {
				mw_push(substr($pushargs[0],1),$pushargs[1]);
			}
		}
		else {
			$loop = 0;
			close(FILE);
		}
	}
}

sub mw_capabilities {
#	print STDOUT "fetch\n";
	print STDOUT "refspec refs/heads/*:refs/mediawiki/$remotename/*\n";
	print STDOUT "import\n";
	print STDOUT "list\n";
	print STDOUT "option\n";
	print STDOUT "push\n";
	print STDOUT "\n";
}

sub mw_list {
	print STDOUT "? refs/heads/master\n";
#	if ($_[0] eq "") { # call to list and not list for push
	print STDOUT '@'."refs/heads/master HEAD\n";
#	}
	print STDOUT "\n";

}

sub mw_option {
	print STDERR "not yet implemented \n";
	print STDOUT "unsupported\n";
}

sub mw_fetch {
	print "\n";
}

sub mw_import {
	
	sub get_last_revision {
		# Get last commit sha1
		my $commit_sha1 = `git rev-parse refs/mediawiki/$remotename/master 2>/dev/null`;

		# Get note regarding that commit
		chomp($commit_sha1);
		my $note = `git notes show $commit_sha1 2>/dev/null`;
		my @note_info = split(/ /, $note);

		my $lastrevision_number;
		if (!($note_info[0] eq "mediawiki_revision:")) {
			print STDERR "No previous mediawiki revision found, fetching from beginning\n";
			$lastrevision_number = 0;
		} else {
			# Notes are formatted : mediawiki_revision: #number
			$lastrevision_number = $note_info[1];
			chomp($lastrevision_number);
			print STDERR "Last mediawiki revision found is $lastrevision_number, fetching from here\n";
		}

		return $lastrevision_number;
	}

	my $url = $_[0];
	my @wiki_name = split(/:\/\//,$url); 
	my $wiki_name = $wiki_name[1];

	my $mediawiki = MediaWiki::API->new;
	$mediawiki->{config}->{api_url} = "$url/api.php";

	my $pages = $mediawiki->list({
		action => 'query',
		list => 'allpages',
		aplimit => 500,
	});

	my @revisions;

	print STDERR "Searching revisions...\n";
	my $fetch_from = get_last_revision() + 1;
	my $n = 1;

	foreach my $page (@$pages) {
		my $id = $page->{pageid};

		print STDERR "$n/", scalar(@$pages), ": $page->{title}\n";
		$n++;

		my $query = {
			action => 'query',
			prop => 'revisions',
			rvprop => 'ids',
			rvdir => 'newer',
			rvstartid => $fetch_from,
			rvlimit => 500,
			pageids => $page->{pageid},
		};

		my $revnum = 0;
		# Get 500 revisions at a time
		while (1) {
			my $result = $mediawiki->api($query);

			# Parse each of those 500 revisions
			foreach my $revision (@{$result->{query}->{pages}->{$id}->{revisions}}) {
				my $page_rev_ids;
				$page_rev_ids->{pageid} = $page->{pageid};
				$page_rev_ids->{revid} = $revision->{revid};
				push (@revisions, $page_rev_ids);
				$revnum++;
			}

			last unless $result->{'query-continue'};
			$query->{rvstartid} = $result->{'query-continue'}->{revisions}->{rvstartid};
			print "\n";
		}

		print STDERR "  Found ", $revnum, " revision(s).\n";

	}


# Creation of the fast-import stream
	print STDERR "Fetching & writing export data...\n";
	binmode STDOUT, ':binary';
	$n = 0;

	foreach my $pagerevids (sort {$a->{revid} <=> $b->{revid}} @revisions) {

		my $query = {
			action => 'query',
			prop => 'revisions',
			rvprop => 'content|timestamp|comment|user|ids',
			revids => $pagerevids->{revid},
		};

		my $result = $mediawiki->api($query);

		my $rev = pop(@{$result->{query}->{pages}->{$pagerevids->{pageid}}->{revisions}});

		$n++;
		my $user = $rev->{user} || 'Anonymous';
		my $dt = DateTime::Format::ISO8601->parse_datetime($rev->{timestamp});

		my $comment = defined $rev->{comment} ? $rev->{comment} : '*Empty MediaWiki Message*';
		my $title = $result->{query}->{pages}->{$pagerevids->{pageid}}->{title};
		my $content = $rev->{'*'};
		$title =~ y/ /_/;

		print STDERR "$n/", scalar(@revisions), ": Revision nÂ°$pagerevids->{revid} of $title\n";
		
		print "commit refs/mediawiki/$remotename/master\n";
		print "mark :$n\n";
		print "committer $user <$user\@$wiki_name> ", $dt->epoch, " +0000\n";
		print "data ", bytes::length(encode_utf8($comment)), "\n", encode_utf8($comment);
		# If it's not a clone, needs to know where to start from
		if ($fetch_from != 1 && $n == 1) {
			print "from refs/mediawiki/$remotename/master^0\n";
		}
		print "M 644 inline $title.wiki\n";
		print "data ", bytes::length(encode_utf8($content)), "\n", encode_utf8($content);
		print "\n\n";
	

		#notes 
		my $note_comment = encode_utf8("note added by git-mediawiki");
		my $note_comment_length = bytes::length($note_comment);
		my $note_content = encode_utf8("mediawiki_revision: " . $pagerevids->{revid} . "\n");
		my $note_content_length = bytes::length($note_content);
		
		if ($fetch_from == 1 && $n == 1) {
			print "reset refs/notes/commits\n";
		}
		print "commit refs/notes/commits\n";
		print "committer $user <user\@example.com> ", $dt->epoch, " +0000\n";
		print "data ", $note_comment_length, "\n", $note_comment;
		if ($fetch_from != 1 && $n == 1) {
			print "from refs/notes/commits^0\n";
		}
		print "N inline :$n\n";
		print "data ", $note_content_length, "\n", $note_content;
		print "\n\n";

	}

	if ($fetch_from == 1) { 
		if ($n != 0) {
			print "reset refs/heads/master\n";
			print "from :$n\n";
		} else {
			print STDERR "You appear to have cloned an empty mediawiki\n";
			#What do we have to do here ? If nothing is done, an error is thrown saying that
			#HEAD is refering to unknown object 0000000000000000000
		}
	}

}

sub mw_push {
	sub get_last_remote_revision {
		my $mediawiki = MediaWiki::API->new;
		$mediawiki->{config}->{api_url} = "$url/api.php";

		my $pages = $mediawiki->list({
			action => 'query',
			list => 'allpages',
			aplimit => 500,
		});

		print STDERR "Searching last remote revision...\n";

		my $max_rev_num = 0;

		foreach my $page (@$pages) {
			my $id = $page->{pageid};


			my $query = {
				action => 'query',
				prop => 'revisions',
				rvprop => 'ids',
				pageids => $id,
			};

			my $result = $mediawiki->api($query);
			
			my $lastrev = pop(@{$result->{query}->{pages}->{$id}->{revisions}});
			
			$max_rev_num = ($lastrev->{revid} > $max_rev_num ? $lastrev->{revid} : $max_rev_num);
		}

		print STDERR "Last remote version found is $max_rev_num\n";
		return $max_rev_num;
	}

	sub push_file {
		print STDERR "Push file : $_[0]\n";
	}
	
	get_last_remote_revision();
	# Get sha1 of commit pointed by local HEAD
	# Get sha1 of commit pointed by remotes/origin/master
	# If they are equal : "Already up to date"
	# Else
		# For each commit in between those two, including the one pointed by HEAD and in chronological order
			# Get files related to this commit
			# Push this file
		# End for each
	# endif

	my $HEAD_sha1 = `git rev-parse HEAD 2>/dev/null`; chomp($HEAD_sha1);
	my $remoteorigin_sha1 = `git rev-parse refs/remotes/origin/master 2>/dev/null`; chomp($remoteorigin_sha1);

	if ($HEAD_sha1 eq $remoteorigin_sha1) {
		print STDERR "Everything up-to-date\n";
	} else {
		#Get every commit in between HEAD and refs/remotes/origin/master, including HEAD and excluding refs/remotes/origin/master
		my @commit_list = ();
		my $n = 0;
		while (1) {
			my $this_sha1 = `git rev-parse HEAD~$n 2>/dev/null`; chomp($this_sha1);
			last unless ($this_sha1 ne $remoteorigin_sha1);
			unshift(@commit_list, $this_sha1);
			$n++;
		}

		foreach my $commit_sha1 (@commit_list) {
			my $tree = `git ls-tree $commit_sha1`;
			my @blob_info_list = split(/\n/, $tree);
			foreach my $blob_info (@blob_info_list) {
				my @blob_info_split = split(/ |\t/, $blob_info);
				push_file($blob_info_split[2]);
			}			
		}
	}
}
