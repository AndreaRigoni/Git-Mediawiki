#! /usr/bin/perl

use strict;
use Switch;
use MediaWiki::API;
use Storable qw(freeze thaw);
use DateTime::Format::ISO8601;
use Encode qw(encode_utf8);

my $remotename = $ARGV[0];
my $url = $ARGV[1];

print STDERR "$url\n";

# commands parser
my $loop = 1;
my $entry;
my @cmd;
while ($loop) {
	$| = 1; #flush STDOUT
	$entry = <STDIN>;
	print STDERR $entry;
	chomp($entry);
	@cmd = undef;
	@cmd = split(/ /,$entry);
	switch ($cmd[0]) {
		case "capabilities" {
			if ($cmd[1] eq "") {
				mw_capabilities();
			} else {
			       $loop = 0;
			}
		}
		case "list" {
			if ($cmd[2] eq "") {
				mw_list($cmd[1]);
			} else {
			       $loop = 0;
			}
		}
		case "import" {
			if ($cmd[1] ne "" && $cmd[2] eq "") {
				mw_import();
			} 
		}
		case "option" {
			mw_option($cmd[1],$cmd[2]);
		}
		case "push" {
			#check the pattern +<src>:<dist>
			my @pushargs = split(/:/,$cmd[1]);
			if ($pushargs[1] ne "" && $pushargs[2] eq "") {
				mw_push($pushargs[0],$pushargs[1]);
			} else {
			       $loop = 0;
			}
		} else {
			$loop = 0;
		}
	}
	close(FILE);
}

########################## Functions ##############################

sub get_last_local_revision {
	# Get last commit sha1
	my $commit_sha1 = `git rev-parse refs/mediawiki/$remotename/master 2>/dev/null`;

	# Get note regarding that commit
	chomp($commit_sha1);
	my $note = `git notes show $commit_sha1 2>/dev/null`;
	my @note_info = split(/ /, $note);

	my $lastrevision_number;
	if (!($note_info[0] eq "mediawiki_revision:")) {
		print STDERR "No previous mediawiki revision found";
		$lastrevision_number = 0;
	} else {
		# Notes are formatted : mediawiki_revision: #number
		$lastrevision_number = $note_info[1];
		chomp($lastrevision_number);
		print STDERR "Last mediawiki revision found is $lastrevision_number ";
	}
	return $lastrevision_number;
}

sub get_last_remote_revision {
	my $mediawiki = MediaWiki::API->new;
	$mediawiki->{config}->{api_url} = "$url/api.php";

	my $pages = $mediawiki->list({
		action => 'query',
		list => 'allpages',
		aplimit => 500,
	});

	my $max_rev_num = 0;

	foreach my $page (@$pages) {
		my $id = $page->{pageid};


		my $query = {
			action => 'query',
			prop => 'revisions',
			rvprop => 'ids',
			pageids => $id,
		};

		my $result = $mediawiki->api($query);
		
		my $lastrev = pop(@{$result->{query}->{pages}->{$id}->{revisions}});
		
		$max_rev_num = ($lastrev->{revid} > $max_rev_num ? $lastrev->{revid} : $max_rev_num);
	}

	print STDERR "Last remote version found is $max_rev_num\n";
	return $max_rev_num;
}

sub mw_capabilities {
	# Revisions are imported to the private namespace
	# refs/mediawiki/$remotename/ by the helper and fetched into
	# refs/remotes/$remotename later by fetch.
	print STDOUT "refspec refs/heads/*:refs/mediawiki/$remotename/*\n";
	print STDOUT "import\n";
	print STDOUT "list\n";
	print STDOUT "option\n";
	print STDOUT "push\n";
	print STDOUT "\n";
}

sub mw_list {
	# MediaWiki do not have branches, we consider one branch arbitrarily
	# called master
	print STDOUT "? refs/heads/master\n";
	print STDOUT '@'."refs/heads/master HEAD\n";
	print STDOUT "\n";

}

sub mw_option {
	print STDERR "not yet implemented \n";
	print STDOUT "unsupported\n";
}

sub mw_import {
	my @wiki_name = split(/:\/\//,$url);
	my $wiki_name = $wiki_name[1];

	my $mediawiki = MediaWiki::API->new;
	$mediawiki->{config}->{api_url} = "$url/api.php";

	my $pages = $mediawiki->list({
		action => 'query',
		list => 'allpages',
		aplimit => 500,
	});
	if ($pages == undef) {
		print STDERR "fatal: '$url' does not appear to be a mediawiki\n";
		exit;
	}

	my @revisions;
	print STDERR "Searching revisions...\n";
	my $fetch_from = get_last_local_revision() + 1;
	if ($fetch_from == 1) {
		print STDERR ", fetching from beginning\n";
	} else {
		print STDERR ", fetching from here\n";
	}
	my $n = 1;
	foreach my $page (@$pages) {
		my $id = $page->{pageid};

		print STDERR "$n/", scalar(@$pages), ": $page->{title}\n";
		$n++;

		my $query = {
			action => 'query',
			prop => 'revisions',
			rvprop => 'ids',
			rvdir => 'newer',
			rvstartid => $fetch_from,
			rvlimit => 500,
			pageids => $page->{pageid},
		};

		my $revnum = 0;
		# Get 500 revisions at a time due to the mediawiki api limit
		while (1) {
			my $result = $mediawiki->api($query);

			# Parse each of those 500 revisions
			foreach my $revision (@{$result->{query}->{pages}->{$id}->{revisions}}) {
				my $page_rev_ids;
				$page_rev_ids->{pageid} = $page->{pageid};
				$page_rev_ids->{revid} = $revision->{revid};
				push (@revisions, $page_rev_ids);
				$revnum++;
			}
			last unless $result->{'query-continue'};
			$query->{rvstartid} = $result->{'query-continue'}->{revisions}->{rvstartid};
			print "\n";
		}
		print STDERR "  Found ", $revnum, " revision(s).\n";
	}

	# Creation of the fast-import stream
	print STDERR "Fetching & writing export data...\n";
	binmode STDOUT, ':binary';
	$n = 0;

	foreach my $pagerevids (sort {$a->{revid} <=> $b->{revid}} @revisions) {
		#fetch the content of the pages
		my $query = {
			action => 'query',
			prop => 'revisions',
			rvprop => 'content|timestamp|comment|user|ids',
			revids => $pagerevids->{revid},
		};

		my $result = $mediawiki->api($query);

		my $rev = pop(@{$result->{query}->{pages}->{$pagerevids->{pageid}}->{revisions}});

		$n++;
		my $user = $rev->{user} || 'Anonymous';
		my $dt = DateTime::Format::ISO8601->parse_datetime($rev->{timestamp});

		my $comment = defined $rev->{comment} ? $rev->{comment} : '*Empty MediaWiki Message*';
		my $title = $result->{query}->{pages}->{$pagerevids->{pageid}}->{title};
		my $content = $rev->{'*'};
		# This \n is VERY important. If it's not added, a conflict is going to happen if you change
		# the last line of a file, push it and then pull it back from mediawiki.
		$content .= "\n"; 
		$title =~ y/ /_/;

		print STDERR "$n/", scalar(@revisions), ": Revision nÂ°$pagerevids->{revid} of $title\n";

		print "commit refs/mediawiki/$remotename/master\n";
		print "mark :$n\n";
		print "committer $user <$user\@$wiki_name> ", $dt->epoch, " +0000\n";
		print "data ", bytes::length(encode_utf8($comment)), "\n", encode_utf8($comment);
		# If it's not a clone, needs to know where to start from
		if ($fetch_from != 1 && $n == 1) {
			print "from refs/mediawiki/$remotename/master^0\n";
		}
		print "M 644 inline $title.wiki\n";
		print "data ", bytes::length(encode_utf8($content)), "\n", encode_utf8($content);
		print "\n\n";


		# mediawiki revision number in the git note
		my $note_comment = encode_utf8("note added by git-mediawiki");
		my $note_comment_length = bytes::length($note_comment);
		my $note_content = encode_utf8("mediawiki_revision: " . $pagerevids->{revid} . "\n");
		my $note_content_length = bytes::length($note_content);

		if ($fetch_from == 1 && $n == 1) {
			print "reset refs/notes/commits\n";
		}
		print "commit refs/notes/commits\n";
		print "committer $user <user\@example.com> ", $dt->epoch, " +0000\n";
		print "data ", $note_comment_length, "\n", $note_comment;
		if ($fetch_from != 1 && $n == 1) {
			print "from refs/notes/commits^0\n";
		}
		print "N inline :$n\n";
		print "data ", $note_content_length, "\n", $note_content;
		print "\n\n";
	}

	if ($fetch_from == 1) {
		if ($n != 0) {
			print "reset refs/heads/master\n";
			print "from :$n\n";
		} else {
			print STDERR "You appear to have cloned an empty mediawiki\n";
			#What do we have to do here ? If nothing is done, an error is thrown saying that
			#HEAD is refering to unknown object 0000000000000000000
		}
	}

}

sub mw_push {

	sub push_file {
		#$_[0] contains a string in this format : 
		#:100644 100644 <sha1_of_blob_before_commit> <sha1_of_blob_now> <status> <filename.wiki>
		my @blob_info_split = split(/ |\t/, $_[0]);

		my $sha1 = $blob_info_split[3];
		my $complete_file_name = $blob_info_split[5];
		my @complete_file_name_split = split(/\./, $complete_file_name);
		my $title = $complete_file_name_split[0];
		my $file_content = `git cat-file -p $sha1`;
		chomp($file_content);

		my $mw = MediaWiki::API->new();
		$mw->{config}->{api_url} = "$url/api.php";

		# log in to the wiki
		#$mw->login( { lgname => 'ilapa', lgpassword => 'kiplaki' } ) || die $mw->{error}->{code} . ': ' . $mw->{error}->{details};
		$mw->edit( {
			action => 'edit',
			summary => 'Commited by git user', #should be changed to the commit message
			title => $title,
			text => encode_utf8($file_content),
		});

		print STDERR "Pushed file : $sha1 - $title\n";
	}
	
	my $last_local_revid = get_last_local_revision();
	my $last_remote_revid = get_last_remote_revision();
	# Get sha1 of commit pointed by local HEAD
	# Get sha1 of commit pointed by remotes/origin/master
	# If they are equal : "Already up to date"
	# Else
		# For each commit in between those two, including the one pointed by HEAD and in chronological order
			# Get files related to this commit
			# Push this file
		# End for each
	# endif

	my $HEAD_sha1 = `git rev-parse $_[0] 2>/dev/null`; chomp($HEAD_sha1);
	my $remoteorigin_sha1 = `git rev-parse refs/remotes/origin/master 2>/dev/null`; chomp($remoteorigin_sha1);

	if ($last_local_revid < $last_remote_revid){
		my $message = "\"To prevent you from losing history, non-fast-forward updates were rejected \\n";
		$message .= "Merge the remote changes (e.g. 'git pull') before pushing again. See the";
		$message .= " 'Note about fast-forwards' section of 'git push --help' for details.\"";
		print "error $_[0] $message\n";
	} elsif ($HEAD_sha1 ne $remoteorigin_sha1) {
		#Get every commit in between HEAD and refs/remotes/origin/master, including HEAD and refs/remotes/origin/master
		my $parsed_sha1 = $remoteorigin_sha1;
		while ($parsed_sha1 ne $HEAD_sha1) {
			my $commit_info =  `git rev-list --children $_[0] | grep -e "^$parsed_sha1"`;
			my @commit_info_split = split(/ |\n/, $commit_info);
			# $commit_info_split[0] is the sha1 of the commit itself
			# $commit_info_split[1] is the sha1 of its direct child
			my $blob_infos = `git diff --raw --abbrev=40 $commit_info_split[0] $commit_info_split[1]`;
			my @blob_info_list = split(/\n/, $blob_infos);
			foreach my $blob_info (@blob_info_list) {
				push_file($blob_info);
			}		
			$parsed_sha1 = $commit_info_split[1];
		}

		print "ok $_[1]\n";

		# Pulling from mediawiki after pushing in order to keep things synchronized
		#exec("git pull >/dev/null");
	}
	print "\n";
}
